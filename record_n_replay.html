<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Record'n replay</title>
  <style>
    #volume-bars {
      display: flex;
      align-items: flex-end;
      height: 256px;
    }
    .bar {
      width: 5px;
      background-color: lightblue;
      margin-right: 1px;
    }
    #recording-status {
      margin-top: 20px;
      font-weight: bold;
    }
    .marker {
      position: relative;
      height: 100%;
      width: 1px;
      background-color: black;
    }
    .marker-label {
      position: absolute;
      top: 260px;
      transform: translateX(-50%);
      font-size: 12px;
    }
    #recognized-word {
      margin-top: 10px;
      font-weight: bold;
    }
    #recordings-list {
      margin-top: 20px;
    }
    .recording-item {
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <button id="start">Activate listening to start recording</button>
  <div id="terms_to_use_chrome">Use Chrome Browser for tested voice recognition.</div>
  <div id="terms_to_speech_recognition">Pressing "Activate" will start sending audio data to browser vendors such as Google. Exit this screen if you don't agree on the terms and functions explained here (<a href="https://developer.chrome.com/blog/voice-driven-web-apps-introduction-to-the-web-speech-api"></a>Voice driven web apps - Introduction to the Web Speech API )</div>
  <div id="volume-bars"></div>
  <div id="recording-status">Recording not started</div>
  <div id="recognized-word">Detected words in audio: <span id="recognized-word-display"></span></div>
  <audio id="audio" controls></audio>
  <div id="recordings-list">History of recordings</div>

  <script>
    const startButton = document.getElementById('start');
    const audioElement = document.getElementById('audio');
    const volumeBars = document.getElementById('volume-bars');
    const recordingStatus = document.getElementById('recording-status');
    const recognizedWordDisplay = document.getElementById('recognized-word-display');
    const recordingsList = document.getElementById('recordings-list');
    let audioChunks = [];
    let mediaRecorder;
    let replayTimeout;
    let recognition;
    let recognitionRestartTimeout;

    // Create 100 volume bars and add markers every 20 bars
    for (let i = 0; i < 100; i++) {
      const bar = document.createElement('div');
      bar.classList.add('bar');
      volumeBars.appendChild(bar);
      if (i % 20 === 0) {
        const marker = document.createElement('div');
        marker.classList.add('marker');
        volumeBars.appendChild(marker);
        const label = document.createElement('div');
        label.classList.add('marker-label');
        label.textContent = i;
        marker.appendChild(label);
      }
    }
    const bars = document.getElementsByClassName('bar');

    async function init() {
      if (alert("Voice data will be sent to Google's server. If you agree on the condition, press 'Ok'")==true){
        return false;
      }
      startButton.disabled = true;
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      mediaRecorder = new MediaRecorder(stream);
      const audioContext = new AudioContext();
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);
      source.connect(analyser);
      analyser.fftSize = 2048;

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) audioChunks.push(event.data);
      };

      mediaRecorder.onstart = () => {
        recordingStatus.textContent = "Recording audio...";
      };

      mediaRecorder.onstop = () => {
        recordingStatus.textContent = "Recording stopped";
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        const audioURL = URL.createObjectURL(audioBlob);

        // Create a new audio element for the recorded audio
        const recordingItem = document.createElement('div');
        recordingItem.classList.add('recording-item');
        const recordedAudio = document.createElement('audio');
        recordedAudio.controls = true;
        recordedAudio.src = audioURL;
        const downloadLink = document.createElement('a');
        downloadLink.href = audioURL;
        downloadLink.download = `recording_${new Date().toISOString()}.webm`;
        downloadLink.textContent = 'Download';

        recordingItem.appendChild(recordedAudio);
        recordingItem.appendChild(downloadLink);
        recordingsList.appendChild(recordingItem);

        // Set the main audio element to the latest recording for replay
        audioElement.src = audioURL;
        audioChunks = [];

        // Start replay after 0.1 second delay
        setTimeout(() => {
          audioElement.play();
          audioElement.onended = () => {
            replayAudioIfNeeded();
          };
        }, 100);
      };

      setupSpeechRecognition(stream);
      detectSound(analyser);
    }

    function setupSpeechRecognition(stream) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.continuous = true;
      recognition.interimResults = true;
      recognition.lang = 'en-US';

      recognition.onresult = (event) => {
        const transcript = event.results[event.results.length - 1][0].transcript.trim().toLowerCase();
        recognizedWordDisplay.textContent = transcript;
        if ((transcript.includes('action') && !transcript.includes('cut')) && mediaRecorder.state === 'inactive') {
          audioElement.pause();
          mediaRecorder.start();
          recognizedWordDisplay.textContent = '';
        } else if (transcript.includes('cut') && mediaRecorder.state === 'recording') {
          mediaRecorder.stop();
          recognizedWordDisplay.textContent = '';
        }
      };

      recognition.onend = () => {
        restartRecognition();
      };

      // Split the playback of the recorded segment and voice recognition input
      const audioContext = new AudioContext();
      const source = audioContext.createMediaStreamSource(stream);
      const destination = audioContext.createMediaStreamDestination();
      source.connect(destination);
      recognition.start();
    }

    function restartRecognition() {
      clearTimeout(recognitionRestartTimeout);
      recognitionRestartTimeout = setTimeout(() => {
        if (recognition && recognition.continuous) {
          recognition.start();
        }
      }, 100);
    }

    function detectSound(analyser) {
      function updateVolumeBars() {
        const dataArray = new Uint8Array(analyser.frequencyBinCount);
        analyser.getByteFrequencyData(dataArray);

        for (let i = 0; i < bars.length; i++) {
          bars[i].style.height = `${dataArray[i]}px`;
        }

        requestAnimationFrame(updateVolumeBars);
      }

      updateVolumeBars();
    }

    function replayAudioIfNeeded() {
      replayTimeout = setTimeout(() => {
        if (audioElement.src && audioElement.paused) {
          audioElement.play();
        }
      }, 1000);
    }

    startButton.addEventListener('click', () => {
      init();
    });

    stopButton.addEventListener('click', () => {
      if (mediaRecorder && mediaRecorder.state === 'recording') {
        mediaRecorder.stop();
      }
    });
  </script>
</body>
</html>
